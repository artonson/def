{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %load_ext line_profiler\n",
    "import h5py\n",
    "import randomcolor\n",
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "from optimization import *\n",
    "from topological_graph import *\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.interpolate import UnivariateSpline, splev, splprep\n",
    "from scipy.optimize import minimize\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import point_cloud_utils as pcu\n",
    "import torch\n",
    "from torch_geometric.nn import fps as tfps\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import ParameterGrid \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from pytorch3d.loss.chamfer import chamfer_distance\n",
    "from geomloss import SamplesLoss\n",
    "import copy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pred_sampling_points(straight_lines,tcks,closed_tcks):\n",
    "    pred_sampling = []\n",
    "\n",
    "    for i in range(len(straight_lines)):\n",
    "        edge = straight_lines[i]\n",
    "        num = int(np.linalg.norm(edge[1] - edge[0]) // 0.001)\n",
    "        if num > 0:\n",
    "            linspace = np.linspace(0, 1, num)\n",
    "        #     break\n",
    "            pred_sampling.append(linspace[:,None] * edge[0] + (1 - linspace)[:,None] * edge[1])\n",
    "        else:\n",
    "            pred_sampling.append(edge)\n",
    "\n",
    "    for i in range(len(tcks)):\n",
    "        pred_sampling.append(np.array(splev(np.linspace(0,1,5000), tcks[i])).T)\n",
    "\n",
    "    for i in range(len(closed_tcks)):    \n",
    "        pred_sampling.append(np.array(splev(np.linspace(0,1,5000), closed_tcks[i])).T)\n",
    "    return pred_sampling\n",
    "\n",
    "def curve_length(curve,devision_points_count=200):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        curve - curve in as in scipy splprep\n",
    "        devision_points_count - the amount of points on curve\n",
    "    output:\n",
    "        length of the curve\n",
    "    \"\"\"\n",
    "    ar = np.array(splev(np.linspace(0,1,devision_points_count), curve)).T\n",
    "    return np.sum(np.sqrt(np.sum((ar[:-1]-ar[1:])**2,axis=1)))\n",
    "\n",
    "def metric_calculation(pred_sampling,gt_sampling,blur=0.05):\n",
    "  \n",
    "    CD_gt_to_pred = (cKDTree(pred_sampling).query(gt_sampling, 1)[0]**2).mean()\n",
    "    CD_pred_to_gt = (cKDTree(gt_sampling).query(pred_sampling, 1)[0]**2).mean()\n",
    "    \n",
    "    pred_sampling = torch.tensor(pred_sampling)[None]\n",
    "    gt_sampling = torch.tensor(gt_sampling)[None]\n",
    "    chamf_l = chamfer_distance(pred_sampling.clone().cuda().detach().float(), gt_sampling.clone().cuda().detach().float())\n",
    "    \n",
    "    if pred_sampling[0].shape[0]>10000:\n",
    "        sub_idx = pcu.downsample_point_cloud_poisson_disk(np.array(pred_sampling[0]), num_samples=10000)\n",
    "        # Use the indices to get the sample positions and normals\n",
    "        pred_sampling_small = pred_sampling[:,sub_idx]\n",
    "    else:\n",
    "        pred_sampling_small =  pred_sampling\n",
    "    \n",
    "    if gt_sampling[0].shape[0]>10000:\n",
    "        sub_idx = pcu.downsample_point_cloud_poisson_disk(np.array(gt_sampling[0]), num_samples=10000)\n",
    "        # Use the indices to get the sample positions and normals\n",
    "        gt_sampling_small = gt_sampling[:,sub_idx]\n",
    "    else:\n",
    "        gt_sampling_small = gt_sampling\n",
    "    sink_ls, en_ls, gaus_ls, lap_ls = loss_calculating(pred_sampling_small,gt_sampling_small,blur=blur)\n",
    "    return CD_gt_to_pred, CD_pred_to_gt,chamf_l, sink_ls, en_ls, gaus_ls, lap_ls\n",
    "\n",
    "def write_results(folder_name,straight_lines,tcks,closed_tcks):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    with open(folder_name + '/vectorization_line.txt', \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(straight_lines, fp)\n",
    "    with open(folder_name+'/vectorization_open_curves.txt', \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(tcks, fp)\n",
    "    with open(folder_name+'/vectorization_closed_curves.txt', \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(closed_tcks, fp)\n",
    "        \n",
    "def read_results(folder_name):\n",
    "    with open(folder_name + '/vectorization_line.txt', \"rb\") as fp:   # Unpickling\n",
    "        straight_lines = pickle.load(fp)\n",
    "    with open(folder_name+'/vectorization_open_curves.txt', \"rb\") as fp:   #Pickling\n",
    "        tcks = pickle.load(fp)\n",
    "    with open(folder_name+'/vectorization_closed_curves.txt', \"rb\") as fp:   #Pickling\n",
    "        closed_tcks = pickle.load(fp)\n",
    "    return straight_lines,tcks,closed_tcks\n",
    "\n",
    "def loss_calculating(pred_sampling_small,inp_sampling_small,blur=0.05):\n",
    "    loss = SamplesLoss(loss=\"sinkhorn\",backend = \"tensorized\",blur=blur)\n",
    "    sink_ls = loss(pred_sampling_small.cuda(), inp_sampling_small.cuda())\n",
    "\n",
    "    loss = SamplesLoss(loss=\"energy\",backend = \"tensorized\",blur=blur)\n",
    "    en_ls = loss(pred_sampling_small.cuda(), inp_sampling_small.cuda())\n",
    "\n",
    "    loss = SamplesLoss(loss=\"gaussian\",backend = \"tensorized\",blur=blur)\n",
    "    gaus_ls = loss(pred_sampling_small.cuda(), inp_sampling_small.cuda())\n",
    "\n",
    "    loss = SamplesLoss(loss=\"laplacian\",backend = \"tensorized\",blur=blur)\n",
    "    lap_ls = loss(pred_sampling_small.cuda(), inp_sampling_small.cuda())\n",
    "\n",
    "    return sink_ls, en_ls, gaus_ls, lap_ls\n",
    "def write_to_file(folder_name, file_name, CD_gt_to_pred, CD_pred_to_gt,chamf_l, sink_ls, en_ls, gaus_ls, lap_ls):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    with open(folder_name +'/'+'metrics_'+ file_name + '.txt', 'w') as f:\n",
    "        f.write('CD_gt_to_pred')\n",
    "        f.write('\\n')\n",
    "        f.write(str(CD_gt_to_pred.item()))\n",
    "        f.write('\\n')\n",
    "        f.write('CD_pred_to_gt')\n",
    "        f.write('\\n')\n",
    "        f.write(str(CD_pred_to_gt.item()))\n",
    "        f.write('\\n')\n",
    "        f.write('Chamfer loss')\n",
    "        f.write('\\n')\n",
    "        f.write(str(chamf_l))\n",
    "        f.write('\\n')\n",
    "        f.write('Energy loss')\n",
    "        f.write('\\n')\n",
    "        f.write(str(en_ls.item()))\n",
    "        f.write('\\n')\n",
    "        f.write('Sinkhorn loss')\n",
    "        f.write('\\n')\n",
    "        f.write(str(sink_ls.item()))\n",
    "        f.write('\\n')\n",
    "        f.write('Gaussian loss')\n",
    "        f.write('\\n')\n",
    "        f.write(str(gaus_ls.item()))\n",
    "        f.write('\\n')\n",
    "        f.write('Laplassian loss')\n",
    "        f.write('\\n')\n",
    "        f.write(str(lap_ls.item()))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# By one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "id_it = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc_0005_00051370_c304c82e5c884522bb82d7af_000'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'pienet_results/'\n",
    "_ids = np.sort(glob('pienet_results/*.hdf5'))\n",
    "# _id = 'abc_0051_00514480_6c33de245ad4c4ff41a3360f_000'\n",
    "# _id = _ids[id_it].split('__')[0][6:]\n",
    "_id = _ids[id_it].split('__')[0][15:-5]\n",
    "_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['closed_curves', 'open_curves', 'point_cloud']\n"
     ]
    }
   ],
   "source": [
    "### print('Processing ', _id)\n",
    "# with h5py.File('{path}/{_id}__min.hdf5'.format(path=path, _id=_id), 'r') as f:\n",
    "with h5py.File('{path}/{_id}.hdf5'.format(path=path, _id=_id), 'r') as f:\n",
    "    print(list(f.keys()))\n",
    "    point_cloud = f['point_cloud'][:]\n",
    "    open_curv = f['open_curves'][:]\n",
    "    close_curv = f['closed_curves'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred_sampling = np.concatenate((np.array(close_curv).reshape(-1,3),np.array(open_curv).reshape(-1,3)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sampling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d2f2125daa4a08b3735a8b63f835b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save this also\n",
    "# if not NO_GT:\n",
    "plot = k3d.plot()\n",
    "\n",
    "cmap=k3d.colormaps.matplotlib_color_maps.plasma_r\n",
    "\n",
    "k3d_points = k3d.points(pred_sampling, point_size=0.03, opacity=0.1, shader='3d', name='sharp_points', color=0xff0000)\n",
    "plot += k3d_points\n",
    "\n",
    "plot.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Writing metrics and curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "NO_GT=False\n",
    "# load GT curves as segments\n",
    "gt_edges = []\n",
    "if os.path.exists('parametric_labeling/{file}__parametric.txt'.format(file=_id[9:])):\n",
    "    with open('parametric_labeling/{file}__parametric.txt'.format(file=_id[9:])) as f:\n",
    "        for line in f:\n",
    "            curve_id, curve_type, edges = line.split(' ', maxsplit=2)\n",
    "            gt_edges.append(np.array([float(v) for v in edges.split()]).reshape((-1, 2, 3)))\n",
    "else:\n",
    "    NO_GT = True\n",
    "    print('No GT!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sample GT edges\n",
    "if not NO_GT:\n",
    "    gt_sampling = []\n",
    "    for edge in np.concatenate(gt_edges):\n",
    "        num = int(np.linalg.norm(edge[1] - edge[0]) // 0.001)\n",
    "        if num > 0:\n",
    "            linspace = np.linspace(0, 1, num)\n",
    "        #     break\n",
    "            gt_sampling.append(linspace[:,None] * edge[0] + (1 - linspace)[:,None] * edge[1])\n",
    "        else:\n",
    "            gt_sampling.append(edge)\n",
    "    gt_sampling = np.concatenate(gt_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not NO_GT:\n",
    "    CD_gt_to_pred, CD_pred_to_gt,chamf_l, sink_ls, en_ls, gaus_ls, lap_ls  = metric_calculation(pred_sampling,gt_sampling)\n",
    "    _, _,chamf_l, sink_ls_bl02, en_ls_bl02, gaus_ls_bl02, lap_ls_bl02  = metric_calculation(pred_sampling,gt_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "folder_name = 'pienet_results'\n",
    "meta_exp_name = folder_name\n",
    "if not NO_GT:\n",
    "    write_to_file(folder_name+'/metrics/',str(_id),CD_gt_to_pred, CD_pred_to_gt,chamf_l, sink_ls, en_ls, gaus_ls, lap_ls)\n",
    "    df = pd.DataFrame([[str(_id),CD_gt_to_pred.item(),CD_pred_to_gt.item(),chamf_l[0].item(),\n",
    "                        en_ls.item(),sink_ls.item(),\n",
    "                        gaus_ls.item(),lap_ls.item(), en_ls_bl02.item(),sink_ls_bl02.item(),\n",
    "                        gaus_ls_bl02.item(),lap_ls_bl02.item()]],\n",
    "                      index=None,  columns=['Name','CD_gt_to_pred','CD_pred_to_gt','Chamfer_loss',\n",
    "                                            'Energy_loss','Sinkhorn_loss','Gaussian_loss'\n",
    "                                            ,'Laplassian_loss','Energy_loss_blure02','Sinkhorn_loss_blure02','Gaussian_loss_blure02'\n",
    "                                            ,'Laplassian_loss_blure02'])\n",
    "    if not os.path.exists(meta_exp_name+'/metrics/'):\n",
    "        os.makedirs(meta_exp_name+'/metrics/')\n",
    "    \n",
    "    if not os.path.exists(meta_exp_name+'/metrics/'+meta_exp_name + '_final_metrics.csv'):\n",
    "        df.to_csv(meta_exp_name+'/metrics/'+meta_exp_name + '_final_metrics.csv', index = False,sep=';')\n",
    "    else:\n",
    "        df.to_csv(meta_exp_name+'/metrics/'+meta_exp_name + '_final_metrics.csv', mode='a', header=None, index = False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e4be4040984767b482f5f1066077ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save this also\n",
    "if not NO_GT:\n",
    "    plot = k3d.plot()\n",
    "\n",
    "    cmap=k3d.colormaps.matplotlib_color_maps.plasma_r\n",
    "\n",
    "    k3d_points = k3d.points(pred_sampling, point_size=0.03, opacity=0.1, shader='3d', name='sharp_points', color=0xff0000)\n",
    "    plot += k3d_points\n",
    "\n",
    "    k3d_points = k3d.points(gt_sampling, point_size=0.03, opacity=0.1, shader='3d', name='sharp_points')\n",
    "    plot += k3d_points\n",
    "\n",
    "    plot.display()\n",
    "    with open(folder_name+'/plot_with_GT/'+str(_id)+'.html', 'w') as f:\n",
    "        f.write(plot.get_snapshot())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# For whole batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 abc_0000_00000006_d4fe04f0f5f84b52bd4f10e4_001\n",
      "1 abc_0000_00000068_767e4372b5f94a88a7a17d90_009\n",
      "2 abc_0000_00000077_767e4372b5f94a88a7a17d90_018\n",
      "3 abc_0000_00000162_4dcda9fab6dd46cf98fb194e_000\n",
      "4 abc_0004_00041400_68770188fbbf4d658868ae7a_000\n",
      "5 abc_0004_00046950_06d3284aaeba4d5e8eb6d441_000\n",
      "6 abc_0005_00051130_fc39aebf81cd429e91825d67_000\n",
      "7 abc_0005_00051370_c304c82e5c884522bb82d7af_000\n",
      "empty_array: 7 abc_0005_00051370_c304c82e5c884522bb82d7af_000\n",
      "8 abc_0005_00057780_da518f0dbee046f287c2acf5_000\n",
      "9 abc_0050_00500041_5aa40dcd43fa0b14df9bdcf8_010\n",
      "10 abc_0050_00500149_54930d6f7740b03347d89a56_000\n",
      "11 abc_0050_00500348_fae0ecd8b3dc068d39f0d09c_000\n",
      "12 abc_0050_00500683_511f7debb63f164003339dec_000\n",
      "13 abc_0050_00500912_ded12ea6d13dadf860aaa4cb_000\n",
      "No GT!\n",
      "14 abc_0050_00507625_b39680dc8dc650ec01956bbe_011\n",
      "No GT!\n",
      "15 abc_0050_00508845_266ea1913e2d35805fb70e9d_001\n",
      "16 abc_0050_00509172_c683c0212890fcb20340e86c_000\n",
      "17 abc_0051_00510038_1509fe78c7a4729189287091_000\n",
      "18 abc_0051_00510073_951e25d2ded40f22b598f84e_000\n",
      "19 abc_0051_00510077_3cb33817515aa6a5e8dd638d_000\n",
      "20 abc_0051_00510118_e88bd32150a7b44caee0fd29_003\n",
      "21 abc_0051_00510141_2b44a82f2bf7942f43634723_000\n",
      "22 abc_0051_00510218_0539d1001509ed5da517b042_000\n",
      "23 abc_0051_00510270_14214b7927680bf60e4a08f4_000\n",
      "24 abc_0051_00510325_0769ac8e70aa984887cf08ec_000\n",
      "25 abc_0051_00510333_44cac8b5175622647f4436a1_000\n",
      "26 abc_0051_00510527_bb814ec02a7f1d6ec7ee7af4_000\n",
      "27 abc_0051_00510545_045b506b58f91329b9fac90a_000\n",
      "28 abc_0051_00510577_71536d4c280b04b7f4a05310_003\n",
      "29 abc_0051_00510848_589a11a3e0d0da0f7abae8a6_003\n",
      "30 abc_0051_00510882_589a14d663f4f81027166c63_001\n",
      "31 abc_0051_00511067_09f6da0fad2ef45888848f34_000\n",
      "32 abc_0051_00511357_6b62490b84f2abcfe801837e_001\n",
      "33 abc_0051_00511469_07a2c0c1151fc7616ad0c39e_000\n",
      "34 abc_0051_00511801_8dd40abaa1cd649cca2577c2_005\n",
      "35 abc_0051_00511846_7b505410617e2dccd4c756dc_013\n",
      "36 abc_0051_00512045_589a74c5e0d0da0f7abb7bad_002\n",
      "37 abc_0051_00512224_589a795c7e864b0f65045295_060\n",
      "38 abc_0051_00512830_589abe4f8f313d0fc5e2666c_004\n",
      "39 abc_0051_00512867_bb8ce171738b5deacb786b2d_008\n",
      "40 abc_0051_00513141_589af21e8460820fe92a2bef_000\n",
      "41 abc_0051_00513464_4990b47baa768a15e5b6f2fa_000\n",
      "42 abc_0051_00513656_61ae550d0c4597033dd4ab99_000\n",
      "43 abc_0051_00513771_589b1e411529400fd28c70ee_069\n",
      "44 abc_0051_00513811_589b21908460820fe92a52ff_000\n",
      "45 abc_0051_00514031_1211c44d63055dd205610531_002\n",
      "46 abc_0051_00514480_6c33de245ad4c4ff41a3360f_000\n",
      "47 abc_0051_00514494_18b4250417be46b2d6fac884_000\n",
      "48 abc_0051_00514605_c19052d705922cd25ac6e93a_005\n",
      "49 abc_0051_00514737_1b29e559d47a5e68f01294db_000\n",
      "empty_array: 49 abc_0051_00514737_1b29e559d47a5e68f01294db_000\n",
      "50 abc_0051_00514829_d05a37c045f46b2cf41639c7_000\n",
      "51 abc_0051_00514908_11c98fc55866ff877cfcdd46_000\n",
      "52 abc_0051_00514939_2e44d41f490e21f75afc031e_000\n",
      "53 abc_0051_00515031_e40ba45a0bd5fbab7ce6b155_000\n",
      "54 abc_0051_00515110_43a0e83aaca82954cacdbe14_005\n",
      "55 abc_0051_00515209_339016deab3d8d5c379667e0_000\n",
      "56 abc_0051_00515357_211afe9d7c17daea9f65e7fa_000\n",
      "57 abc_0051_00515501_91a84e96f5d5519cd3938dcb_000\n",
      "58 abc_0051_00515566_589bb9e8e1c77e0ffe2fcb60_023\n",
      "59 abc_0051_00515662_08a4c17e669921ce4fc67435_001\n",
      "60 abc_0051_00515671_589bc577e991810f6c5530f6_006\n",
      "61 abc_0051_00515738_6a626f99dbc0bbfd0c786022_000\n",
      "62 abc_0051_00515778_efcde9d8e66cecd7fd2f4013_004\n",
      "63 abc_0051_00515797_589bdec38006660f7f91d8e2_001\n",
      "64 abc_0051_00515801_589bdec38006660f7f91d8e2_005\n",
      "65 abc_0051_00515868_589bdec38006660f7f91d8e2_072\n",
      "66 abc_0051_00515869_589bdec38006660f7f91d8e2_073\n",
      "67 abc_0051_00515877_589bdec38006660f7f91d8e2_081\n",
      "68 abc_0051_00515913_f2047825d47c9e1474ad63fa_000\n",
      "69 abc_0051_00515956_654ec074c8d83a0b57e46110_004\n",
      "70 abc_0051_00516159_9277ec867147964cea5ea57d_000\n",
      "71 abc_0051_00516184_e61c9fb26582c5967d218009_000\n",
      "72 abc_0051_00516341_f2762df970f5541acaf37038_000\n",
      "73 abc_0051_00516364_1bb8f06910990563c3e12991_000\n",
      "74 abc_0051_00516398_3d8f5ccaf54e6a12ab4a2153_000\n",
      "75 abc_0051_00516399_3d8f5ccaf54e6a12ab4a2153_001\n",
      "76 abc_0051_00516407_36ff287f834a1627a4b5d1c9_000\n",
      "77 abc_0051_00516689_4581cda73ff1f27569d69fc6_002\n",
      "78 abc_0051_00516874_589c9226d978e50f89dfb381_008\n",
      "79 abc_0051_00516985_701b01041577737beea11fc2_003\n",
      "80 abc_0051_00516996_76ad2d8473bb693d8a9c70db_000\n",
      "81 abc_0051_00517328_d8d7899e536c5464c1b02340_000\n",
      "82 abc_0051_00517412_589cc2d908d6170f75acb3e2_002\n",
      "83 abc_0051_00517548_f13866b5bc2af32ab038252f_000\n",
      "84 abc_0051_00517560_6b1e65e999a752d65261fccb_000\n",
      "85 abc_0051_00517689_6fe915d6aada5c49c7c7ed68_000\n",
      "86 abc_0051_00517760_589ce1e83a1df80f7b3b569c_005\n",
      "87 abc_0051_00517764_589ce2c308d6170f75ad0302_000\n",
      "88 abc_0051_00517778_112c441b377c9f99f5849ff2_000\n",
      "89 abc_0051_00517786_c8745c064381ae55fad72001_004\n",
      "90 abc_0051_00517796_836c045df81452a9654b13ef_001\n",
      "91 abc_0051_00517802_486ace821ef499b593f72d64_000\n",
      "92 abc_0051_00517949_aabc24344bb566626be4bd02_000\n",
      "93 abc_0051_00517983_0bf71c8f7dd022fab51b8c00_000\n",
      "94 abc_0051_00518757_621df8ea8dc4ef8aa2c5030b_002\n"
     ]
    }
   ],
   "source": [
    "folder_name = 'pienet_results'\n",
    "for id_it in range(0,95):\n",
    "    path = 'pienet_results/'\n",
    "    _ids = np.sort(glob('pienet_results/*.hdf5'))\n",
    "    _id = _ids[id_it].split('__')[0][15:-5]\n",
    "    print(id_it,_id)\n",
    "    with h5py.File('{path}/{_id}.hdf5'.format(path=path, _id=_id), 'r') as f:\n",
    "        point_cloud = f['point_cloud'][:]\n",
    "        open_curv = f['open_curves'][:]\n",
    "        close_curv = f['closed_curves'][:]\n",
    "    pred_sampling = np.concatenate((np.array(close_curv).reshape(-1,3),np.array(open_curv).reshape(-1,3)),axis=0)\n",
    "    if pred_sampling.size == 0:\n",
    "        print('empty_array:', id_it,_id)\n",
    "        continue\n",
    "    NO_GT=False\n",
    "    # load GT curves as segments\n",
    "    gt_edges = []\n",
    "    if os.path.exists('parametric_labeling/{file}__parametric.txt'.format(file=_id[9:])):\n",
    "        with open('parametric_labeling/{file}__parametric.txt'.format(file=_id[9:])) as f:\n",
    "            for line in f:\n",
    "                curve_id, curve_type, edges = line.split(' ', maxsplit=2)\n",
    "                gt_edges.append(np.array([float(v) for v in edges.split()]).reshape((-1, 2, 3)))\n",
    "    else:\n",
    "        NO_GT = True\n",
    "        print('No GT!')\n",
    "    \n",
    "    # sample GT edges\n",
    "    if not NO_GT:\n",
    "        gt_sampling = []\n",
    "        for edge in np.concatenate(gt_edges):\n",
    "            num = int(np.linalg.norm(edge[1] - edge[0]) // 0.001)\n",
    "            if num > 0:\n",
    "                linspace = np.linspace(0, 1, num)\n",
    "            #     break\n",
    "                gt_sampling.append(linspace[:,None] * edge[0] + (1 - linspace)[:,None] * edge[1])\n",
    "            else:\n",
    "                gt_sampling.append(edge)\n",
    "        gt_sampling = np.concatenate(gt_sampling)\n",
    "    if not NO_GT:\n",
    "        CD_gt_to_pred, CD_pred_to_gt,chamf_l, sink_ls, en_ls, gaus_ls, lap_ls  = metric_calculation(pred_sampling,gt_sampling)\n",
    "        _, _,chamf_l, sink_ls_bl02, en_ls_bl02, gaus_ls_bl02, lap_ls_bl02  = metric_calculation(pred_sampling,gt_sampling)\n",
    "    \n",
    "    meta_exp_name = folder_name\n",
    "    if not NO_GT:\n",
    "        write_to_file(folder_name+'/metrics/',str(_id),CD_gt_to_pred, CD_pred_to_gt,chamf_l, sink_ls, en_ls, gaus_ls, lap_ls)\n",
    "        df = pd.DataFrame([[str(_id),CD_gt_to_pred.item(),CD_pred_to_gt.item(),chamf_l[0].item(),\n",
    "                            en_ls.item(),sink_ls.item(),\n",
    "                            gaus_ls.item(),lap_ls.item(), en_ls_bl02.item(),sink_ls_bl02.item(),\n",
    "                            gaus_ls_bl02.item(),lap_ls_bl02.item()]],\n",
    "                          index=None,  columns=['Name','CD_gt_to_pred','CD_pred_to_gt','Chamfer_loss',\n",
    "                                                'Energy_loss','Sinkhorn_loss','Gaussian_loss'\n",
    "                                                ,'Laplassian_loss','Energy_loss_blure02','Sinkhorn_loss_blure02','Gaussian_loss_blure02'\n",
    "                                                ,'Laplassian_loss_blure02'])\n",
    "        if not os.path.exists(meta_exp_name+'/metrics/'):\n",
    "            os.makedirs(meta_exp_name+'/metrics/')\n",
    "\n",
    "        if not os.path.exists(meta_exp_name+'/metrics/'+meta_exp_name + '_final_metrics.csv'):\n",
    "            df.to_csv(meta_exp_name+'/metrics/'+meta_exp_name + '_final_metrics.csv', index = False,sep=';')\n",
    "        else:\n",
    "            df.to_csv(meta_exp_name+'/metrics/'+meta_exp_name + '_final_metrics.csv', mode='a', header=None, index = False,sep=';')\n",
    "        # save this also\n",
    "    \n",
    "    if not NO_GT:\n",
    "        plot = k3d.plot()\n",
    "\n",
    "        cmap=k3d.colormaps.matplotlib_color_maps.plasma_r\n",
    "\n",
    "        k3d_points = k3d.points(pred_sampling, point_size=0.03, opacity=0.1, shader='3d', name='sharp_points', color=0xff0000)\n",
    "        plot += k3d_points\n",
    "\n",
    "        k3d_points = k3d.points(gt_sampling, point_size=0.03, opacity=0.1, shader='3d', name='sharp_points')\n",
    "        plot += k3d_points\n",
    "\n",
    "#         plot.display()\n",
    "        with open(folder_name+'/plot_with_GT/'+str(_id)+'.html', 'w') as f:\n",
    "            f.write(plot.get_snapshot())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}