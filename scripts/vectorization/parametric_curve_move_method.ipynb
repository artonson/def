{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %load_ext line_profiler\n",
    "import h5py\n",
    "import randomcolor\n",
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "from optimization import *\n",
    "from topological_graph import *\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.interpolate import UnivariateSpline, splev, splprep\n",
    "from scipy.optimize import minimize\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import point_cloud_utils as pcu\n",
    "import torch\n",
    "from torch_geometric.nn import fps as tfps\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import ParameterGrid \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from pytorch3d.loss.chamfer import chamfer_distance\n",
    "from geomloss import SamplesLoss\n",
    "import copy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "RES = 0.02\n",
    "sharpness_threshold = RES * 3 #RES*3\n",
    "\n",
    "knn_radius = 2\n",
    "\n",
    "filtering_radius = RES * 1 # max distance to connect a pair in knn filtering\n",
    "corner_connected_components_radius = RES * knn_radius # max distance to connect a pair in knn corner separation\n",
    "curve_connected_components_radius = RES * 2.5 # max distance to connect a pair in knn curve separation\n",
    "\n",
    "sampling = 'blue' # 'blue','random','voxel'\n",
    "num_voxels_per_axis = 100\n",
    "subsample_factor = 10\n",
    "filtering_factor = 30\n",
    "filtering_mode = False\n",
    "fps_factor = 5\n",
    "\n",
    "# corner_detector_radius = RES * 8\n",
    "# upper_variance_threshold = 0.05\n",
    "# lower_variance_threshold = 0.04\n",
    "cornerness_threshold = 0.7\n",
    "# box_margin = RES * 4\n",
    "quantile = 1\n",
    "\n",
    "curve_extraction_mode = 'knn' # 'subdivision', 'knn'\n",
    "endpoint_detector_radius = RES * 10\n",
    "endpoint_threshold = 0.6\n",
    "\n",
    "corner_connector_radius = RES * 20\n",
    "\n",
    "initial_split_threshold = RES * 4\n",
    "# optimization_split_threshold = RES * 3\n",
    "alpha_ang = 1000\n",
    "\n",
    "DISPLAY_RES = 0.03\n",
    "cut_metric = 'fscore' #fscore #'chamfer_dist','sinkhorn' \n",
    "\n",
    "\n",
    "small_lines_clip_primitive_number = 3\n",
    "small_lines_clip_primitives_length = 0.4\n",
    "\n",
    "id_it = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc_0050_00500041_5aa40dcd43fa0b14df9bdcf8_010'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'input/'\n",
    "_ids = np.sort(glob('input/*.hdf5'))\n",
    "# _id = 'abc_0051_00514480_6c33de245ad4c4ff41a3360f_000'\n",
    "_id = _ids[id_it].split('__')[0][6:]\n",
    "_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc_0051_00517802_486ace821ef499b593f72d64_000'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path = 'input_new_models/'\n",
    "# _ids = np.sort(glob('input_new_models/*.hdf5'))\n",
    "# # _id = 'abc_0051_00514480_6c33de245ad4c4ff41a3360f_000'\n",
    "# # _id = _ids[id_it].split('__')[0][6:]\n",
    "# _id = _ids[id_it].split('__')[0][17:]\n",
    "# _id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_exp_name = 'cor_' + str(cornerness_threshold) + '_sharp_' + str(sharpness_threshold)+'_subs_'+ str(subsample_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(meta_exp_name):\n",
    "    os.makedirs(meta_exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#changed\n",
    "folder_name = 'id_it' + str(id_it) + '_'+ str(_id)+'_cor_' + str(cornerness_threshold) + '_sharp_' + str(sharpness_threshold)+'_subs_'+ str(subsample_factor)\n",
    "if not os.path.exists(meta_exp_name+'/'+folder_name):\n",
    "    os.makedirs(meta_exp_name+ '/'+folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182904, 3)\n",
      "subsample_rate 18290\n",
      "processing 182904 points\n",
      "blue sampling\n",
      "2.379896879196167\n",
      "processing 17925 points\n",
      "2.380798578262329\n"
     ]
    }
   ],
   "source": [
    "### print('Processing ', _id)\n",
    "# with h5py.File('{path}/{_id}__min.hdf5'.format(path=path, _id=_id), 'r') as f:\n",
    "with h5py.File('{path}/{_id}__adv60.hdf5'.format(path=path, _id=_id), 'r') as f:\n",
    "    whole_model_points = f['points'][:]\n",
    "    whole_model_distances = f['distances'][:]\n",
    "points = whole_model_points[whole_model_distances < sharpness_threshold]\n",
    "distances = whole_model_distances[whole_model_distances < sharpness_threshold]\n",
    "print(points.shape)\n",
    "subsample_rate = int(points.shape[0] / subsample_factor)\n",
    "print(\"subsample_rate\", subsample_rate)\n",
    "\n",
    "print('processing {size} points'.format(size=len(points)))\n",
    "start_all = time.time()\n",
    "if sampling:\n",
    "    if  subsample_rate > 0 and sampling=='random':\n",
    "        print('random sampling')\n",
    "        sub_idx = np.random.choice(np.arange(len(points)), subsample_rate)\n",
    "        points = points[sub_idx]\n",
    "        distances = distances[sub_idx]\n",
    "    elif sampling=='blue'and subsample_rate > 0:\n",
    "        start =time.time()\n",
    "        print('blue sampling')\n",
    "        # Downsample a point cloud by approximately to subsample_rate so that the sampled points approximately\n",
    "        # follow a blue noise distribution\n",
    "        # idx is an array of integer indices into v indicating which samples to keep\n",
    "        sub_idx = pcu.downsample_point_cloud_poisson_disk(points, num_samples=subsample_rate)\n",
    "        # Use the indices to get the sample positions and normals\n",
    "        points = points[sub_idx]\n",
    "        distances = distances[sub_idx]\n",
    "        print(time.time()-start)\n",
    "    elif sampling=='voxel':\n",
    "        start =time.time()\n",
    "        print(\"voxel sampling\")\n",
    "        # We'll use a voxel grid with 128 voxels per axis\n",
    "        distances = np.array(np.tile(distances, (3,1)).transpose(1,0), order='C')\n",
    "\n",
    "        # Size of the axis aligned bounding box of the point cloud\n",
    "        bbox_size = points.max(0) - points.min(0)\n",
    "\n",
    "        # The size per-axis of a single voxel\n",
    "        sizeof_voxel = bbox_size / num_voxels_per_axis\n",
    "\n",
    "        # Downsample a point cloud on a voxel grid so there is at most one point per voxel.\n",
    "        # Multiple points, normals, and colors within a voxel cell are averaged together.\n",
    "        v_sampled, n_sampled, c_sampled = pcu.downsample_point_cloud_voxel_grid(sizeof_voxel, points,distances , None)\n",
    "        points = v_sampled\n",
    "        distances = n_sampled[:,0]\n",
    "        print(time.time()-start)\n",
    "    else:\n",
    "        raise Exception(\"No other option for samoling\") \n",
    "\n",
    "\n",
    "if filtering_mode:\n",
    "    print('filtering')\n",
    "    filtered_clusters = separate_graph_connected_components(points, radius=filtering_radius, filtering_mode=True, \n",
    "                                                            filtering_factor=filtering_factor)\n",
    "    points = points[np.unique(np.concatenate(filtered_clusters))]\n",
    "    distances = distances[np.unique(np.concatenate(filtered_clusters))]\n",
    "\n",
    "print('processing {size} points'.format(size=len(points)))\n",
    "# fps = farthest_point_sampling(points, points.shape[0] // fps_factor)\n",
    "# print('fps_finished')\n",
    "print(time.time()-start_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13739466667175293\n",
      "1.2987494468688965\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "a = torch.tensor(points)\n",
    "print(time.time()-start)\n",
    "start = time.time()\n",
    "fps = tfps(a,ratio=1/fps_factor)\n",
    "print(time.time()-start)\n",
    "fps = fps.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'corner_detector_radius':[RES * 5, RES * 6, RES * 7, RES * 8],\n",
    "              'neighbours_count':[10,20,40], \n",
    "              'variance_dynamics_threshold':[5,10,15,20,25]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighb_prob(par, variance):\n",
    "    var_dynamics = np.zeros_like(variances)\n",
    "    tree_fps = cKDTree(points[fps])\n",
    "    fps_nbhds = tree_fps.query(points[fps], par['neighbours_count'])\n",
    "    \n",
    "    for i in range(1,par['neighbours_count']):\n",
    "        var_dynamics += ((variances[fps_nbhds[1][:,i]] - variances) / fps_nbhds[0][:,i][:,None]) ** 2\n",
    "#     var_dynamics = np.abs(var_dynamics)\n",
    "    # it can be done softer, can be just averaging var_dinamics\n",
    "    return np.where(var_dynamics.sum(-1) > par['variance_dynamics_threshold'])\n",
    "    #return var_dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3585/3585 [00:01<00:00, 1976.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:01<00:00, 10.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3585/3585 [00:01<00:00, 3229.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 1024.30it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3585/3585 [00:01<00:00, 3159.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 1130.95it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3585/3585 [00:01<00:00, 3003.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 1201.23it/s]\n"
     ]
    }
   ],
   "source": [
    "boxes = []\n",
    "for it in range(len(parameters['corner_detector_radius'])):\n",
    "    tree = cKDTree(points)\n",
    "    neighbours = tree.query_ball_point(points[fps], r=parameters[\"corner_detector_radius\"][it])\n",
    "    variances = []\n",
    "    for n in tqdm(neighbours):\n",
    "        pca = PCA(3)\n",
    "        # if size of neighbourhood is sufficient\n",
    "        if len(n) > 5:\n",
    "            pca.fit(points[n])\n",
    "            variances.append(pca.explained_variance_ratio_)\n",
    "        else: variances.append([1,0,0])\n",
    "    variances = np.array(variances)\n",
    "    keys = ['neighbours_count','variance_dynamics_threshold' ]\n",
    "    cicle_par = {x:parameters[x] for x in keys}\n",
    "    #TODO can be optimized \n",
    "    boxsss=Parallel(n_jobs=4)(delayed(neighb_prob)(par,variances) for par in tqdm(ParameterGrid(cicle_par)))\n",
    "    boxes.append(boxsss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "(corners,counts) = np.unique(np.concatenate(np.concatenate(np.concatenate(boxes))),return_counts=True)\n",
    "prob = counts/(len(list(ParameterGrid(cicle_par)))*len(parameters['corner_detector_radius']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/v.egiazarian/environments/envs/lib/python3.8/site-packages/traittypes/traittypes.py:97: UserWarning: Given trait value dtype \"float64\" does not match required type \"float32\". A coerced copy has been created.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96cad44fe804265beb219eaa3a42861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = k3d.plot()\n",
    "\n",
    "cmap=k3d.colormaps.matplotlib_color_maps.plasma\n",
    "colors = k3d.helpers.map_colors(\n",
    "                prob, cmap, [prob.min(), prob.max()]\n",
    "            ).astype(np.uint32)\n",
    "\n",
    "k3d_points = k3d.points(points[fps][corners.astype(int)], point_size=DISPLAY_RES, shader='3d', name='sharp_points',colors=colors)\n",
    "plot += k3d_points\n",
    "\n",
    "k3d_points_i = k3d.points(points[fps][~np.in1d(range(len(points[fps])),corners)], point_size=DISPLAY_RES, shader='3d', name='sharp_points')\n",
    "plot += k3d_points_i\n",
    "\n",
    "plot.display()\n",
    "\n",
    "with open(meta_exp_name+'/'+folder_name+'/'+'prob_corr'+'.html', 'w') as f:\n",
    "    f.write(plot.get_snapshot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fps_prob = np.zeros(points[fps].shape[0])\n",
    "all_fps_prob[corners] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsRegressor(n_neighbors=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.fit(points[fps], all_fps_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# не знаю стоит ли предсказывать для всех или только для тех которые не известны\n",
    "all_prob = neigh.predict(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cornerness_threshold=0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cornerness_threshold=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corners = np.arange(points.shape[0])[all_prob>cornerness_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornerness_threshold=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67620d44ad804856be5265612ec74d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = k3d.plot()\n",
    "\n",
    "\n",
    "k3d_points = k3d.points(points[np.setdiff1d(np.arange(len(points)), corners)], point_size=DISPLAY_RES, shader='3d', name='sharp_points')\n",
    "plot += k3d_points\n",
    "\n",
    "k3d_points = k3d.points(points[corners], point_size=DISPLAY_RES, shader='3d', name='sharp_points',color=0xff0000)\n",
    "plot += k3d_points\n",
    "\n",
    "\n",
    "plot.display()\n",
    "with open(meta_exp_name + '/'+ folder_name +'/'+'corners'+'.html', 'w') as f:\n",
    "    f.write(plot.get_snapshot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decrease if neccessary (if there are glued corners)\n",
    "# quantile = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:00, 29.10it/s]\n"
     ]
    }
   ],
   "source": [
    "corner_centers = []\n",
    "corner_clusters = []\n",
    "corners = np.arange(points.shape[0])[all_prob>cornerness_threshold]\n",
    "\n",
    "tmp_corner_clusters = separate_graph_connected_components(points[corners], corner_connected_components_radius,\n",
    "                                                                          compute_barycenters=False)\n",
    "tmp_corner_centers = [np.median(points[corners][clust], axis=0).tolist() for clust in tmp_corner_clusters] \n",
    "\n",
    "# for every corner box, sample two points as corners, and store connections between them\n",
    "\n",
    "init_connections = []\n",
    "norms = []\n",
    "for i, cluster in enumerate(tmp_corner_clusters):\n",
    "    tmp_p = points[corners][cluster]\n",
    "    tmp_p -= tmp_p.mean(0)\n",
    "    norms.append(np.linalg.norm(tmp_p, axis=1).max())\n",
    "for i, cluster in enumerate(tmp_corner_clusters):\n",
    "    if norms[i] < np.quantile(norms, quantile):\n",
    "        corner_clusters.append(cluster)\n",
    "        query = cKDTree(points[corners]).query(tmp_corner_centers[i], 1)\n",
    "        corner_centers.append(query[1])\n",
    "        continue\n",
    "    else:\n",
    "        gmm = GaussianMixture(2)\n",
    "        res = gmm.fit_predict(points[corners][cluster])\n",
    "        corner_clusters.append(np.array(cluster)[res == 0].tolist())\n",
    "        corner_clusters.append(np.array(cluster)[res == 1].tolist())\n",
    "        endpoints_query = cKDTree(points[corners]).query(gmm.means_, 1)\n",
    "        ind = len(corner_centers)\n",
    "        corner_centers.append(endpoints_query[1][0])\n",
    "        corner_centers.append(endpoints_query[1][1])\n",
    "        init_connections.append([ind, ind+1])    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:02,  6.20it/s]\n"
     ]
    }
   ],
   "source": [
    "not_corners = np.setdiff1d(np.arange(len(points)), corners)\n",
    "\n",
    "# print('separating curves')\n",
    "curves = separate_graph_connected_components(points[not_corners], radius=curve_connected_components_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/v.egiazarian/environments/envs/lib/python3.8/site-packages/traittypes/traittypes.py:97: UserWarning: Given trait value dtype \"float64\" does not match required type \"float32\". A coerced copy has been created.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0606e4471fc24170a13723cab34eaacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = k3d.plot()\n",
    "rand_color = randomcolor.RandomColor()\n",
    "\n",
    "cmap=k3d.colormaps.matplotlib_color_maps.plasma_r\n",
    "colors = k3d.helpers.map_colors(\n",
    "                distances, cmap, [distances.min(), distances.max()]\n",
    "            ).astype(np.uint32)\n",
    "\n",
    "plot += k3d.points(points, point_size=0.03, colors=colors)\n",
    "\n",
    "for i in corner_clusters:\n",
    "    color = rand_color.generate()[0]\n",
    "    color = int('0x' + color[1:], 16)\n",
    "    plot += k3d.points(points[corners][i], color=color, point_size=0.03)\n",
    "    \n",
    "for i in range(len(curves)):\n",
    "    color = rand_color.generate()[0]\n",
    "    color = int('0x' + color[1:], 16)\n",
    "    plot += k3d.points(points[not_corners][curves[i]], color=color, point_size=0.03)\n",
    "    \n",
    "plot.display()\n",
    "with open(meta_exp_name+'/'+folder_name+'/'+'segmentation_'+curve_extraction_mode+'.html', 'w') as f:\n",
    "    f.write(plot.get_snapshot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curve_extraction_mode = 'knn' #subdivision,knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing topological graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  4.11it/s]\n"
     ]
    }
   ],
   "source": [
    "print('initializing topological graph')\n",
    "from topological_graph import *\n",
    "corner_positions, corner_pairs = initialize_topological_graph(points, distances, \n",
    "                                                              not_corners, curves, \n",
    "                                                              corners, corner_centers,\n",
    "                                                              init_connections,\n",
    "                                                              endpoint_detector_radius, endpoint_threshold, \n",
    "                                                              initial_split_threshold, corner_connector_radius,\n",
    "                                                              curve_extraction_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.arange(len(corner_positions))\n",
    "not_corners_ss = np.setdiff1d(np.arange(len(np.arange(len(corner_positions)))), np.unique(corner_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/v.egiazarian/environments/envs/lib/python3.8/site-packages/traittypes/traittypes.py:97: UserWarning: Given trait value dtype \"float64\" does not match required type \"float32\". A coerced copy has been created.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61c44195264419c9435cd5878eb868a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = k3d.plot()\n",
    "\n",
    "k3d_points = k3d.points(points, point_size=DISPLAY_RES, opacity=0.1, shader='3d', name='sharp_points')\n",
    "plot += k3d_points\n",
    "\n",
    "points_corner_centers = k3d.points(np.array(corner_positions),\n",
    "                                   color=0xFF0000, point_size=DISPLAY_RES, shader='3d', name='polyline_nodes')\n",
    "plot += points_corner_centers\n",
    "\n",
    "for edge in corner_pairs:\n",
    "    try:\n",
    "        e = k3d.line(np.array(corner_positions)[edge], name='polyline_edge')\n",
    "        plot+=e\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "plot.display()\n",
    "\n",
    "\n",
    "with open(meta_exp_name+'/'+folder_name+'/'+'graph_initialization_'+curve_extraction_mode+'.html', 'w') as f:\n",
    "    f.write(plot.get_snapshot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizing topological graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization: step #149, fit loss: 6.6, angle loss: -18.9: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [00:06<00:00, 23.71it/s]\n",
      "Optimization: step #149, fit loss: 6.0, angle loss: -18.8: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [00:06<00:00, 24.79it/s]\n"
     ]
    }
   ],
   "source": [
    "print('optimizing topological graph')\n",
    "from optimization import *\n",
    "corner_positions, corner_pairs = optimize_topological_graph(corner_positions, corner_pairs, corner_centers, \n",
    "                                                            points, distances, \n",
    "                                                            initial_split_threshold, alpha_fit=1, \n",
    "                                                            alpha_ang=1, corner_alpha_ang=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abea107e3691410f867c2cf0a241b496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = k3d.plot()\n",
    "\n",
    "k3d_points = k3d.points(points, point_size=DISPLAY_RES, opacity=0.1, shader='3d', name='sharp_points')\n",
    "plot += k3d_points\n",
    "\n",
    "points_corner_centers = k3d.points(np.array(corner_positions),\n",
    "                                   color=0xFF0000, point_size=DISPLAY_RES, shader='3d', name='polyline_nodes')\n",
    "plot += points_corner_centers\n",
    "\n",
    "for edge in corner_pairs:\n",
    "    try:\n",
    "        e = k3d.line(np.array(corner_positions)[edge], name='polyline_edge')\n",
    "        plot+=e\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "plot.display()\n",
    "\n",
    "\n",
    "with open(meta_exp_name+'/'+folder_name+'/'+'graph_optimization_'+curve_extraction_mode+'.html', 'w') as f:\n",
    "    f.write(plot.get_snapshot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating paths\n",
      "aabboxing\n"
     ]
    }
   ],
   "source": [
    "from optimization import *\n",
    "print('creating paths')\n",
    "_, paths, _, closed_paths, _ = get_paths_and_corners(corner_pairs, corner_positions, corner_centers)\n",
    "\n",
    "\n",
    "print('aabboxing')\n",
    "# aabboxes = create_aabboxes(np.array(corner_positions)[np.array(corner_pairs)])\n",
    "matching = parallel_nearest_point(np.array(corner_positions), np.array(corner_pairs), points)\n",
    "curve_data, curve_distances = recalculate(points, distances, matching, corner_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curve_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils for straight segments detection\n",
    "\n",
    "straight_threshold = 0.977\n",
    "breakpoint_threshold = 0.2\n",
    "\n",
    "path_angles = []\n",
    "for path in paths:\n",
    "    path_pairs = list(zip(path, path[1:]))\n",
    "    current_angles = []\n",
    "    if len(path_pairs) == 1:\n",
    "        path_angles.append(None)\n",
    "        continue\n",
    "    for path_node in range(1,len(path) - 1):\n",
    "        vec1 = np.array(corner_positions)[path[path_node]] - np.array(corner_positions)[path[0]]\n",
    "        vec2 = np.array(corner_positions)[path[-1]] - np.array(corner_positions)[path[path_node]]\n",
    "        vec1 /= np.linalg.norm(vec1)\n",
    "        vec2 /= np.linalg.norm(vec2)\n",
    "        current_angles.append(np.dot(vec1, vec2))\n",
    "    path_angles.append(current_angles)\n",
    "    \n",
    "straight = np.zeros((len(paths)))\n",
    "for i in range(len(paths)):\n",
    "    if len(paths[i]) == 2:\n",
    "        straight[i] = 1\n",
    "    if path_angles[i] is not None:\n",
    "        if (np.array(path_angles[i]) >= straight_threshold).all():\n",
    "            straight[i] = 1\n",
    "\n",
    "path_angles = []\n",
    "for path in paths:\n",
    "    path_pairs = list(zip(path, path[1:]))\n",
    "    current_angles = []\n",
    "    if len(path_pairs) == 1:\n",
    "        path_angles.append(None)\n",
    "        continue\n",
    "    for path_node in range(1,len(path) - 1):\n",
    "        vec1 = np.array(corner_positions)[path[path_node]] - np.array(corner_positions)[path[path_node-1]]\n",
    "        vec2 = np.array(corner_positions)[path[path_node+1]] - np.array(corner_positions)[path[path_node]]\n",
    "        vec1 /= np.linalg.norm(vec1)\n",
    "        vec2 /= np.linalg.norm(vec2)\n",
    "        current_angles.append(np.dot(vec1, vec2))\n",
    "    path_angles.append(current_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "new_paths = []\n",
    "for i,path in enumerate(paths):\n",
    "    if path_angles[i] is None:\n",
    "        new_paths.append(path)\n",
    "        continue\n",
    "    if straight[i] == 1:\n",
    "        new_paths.append(path)\n",
    "        continue\n",
    "    if (np.array(path_angles[i]) > straight_threshold).any() and (np.array(path_angles[i]) < breakpoint_threshold).any():\n",
    "        breakpoints = []\n",
    "        for j in range(1,len(path)-2):\n",
    "            if (np.array(path_angles[i]) > straight_threshold)[j-1] == (np.array(path_angles[i]) < breakpoint_threshold)[j]:\n",
    "                breakpoints.append((np.array([j-1,j]) * np.logical_not([(np.array(path_angles[i]) > straight_threshold)[j-1], (np.array(path_angles[i]) > breakpoint_threshold)[j]])).sum())\n",
    "        if not breakpoints:\n",
    "            continue\n",
    "        breakpoints = np.unique(breakpoints)\n",
    "        new_paths.append(path[:breakpoints[0]+2])\n",
    "        new_paths.append(path[breakpoints[-1]+1:])\n",
    "        if len(breakpoints) > 1:\n",
    "            for b,c in zip(breakpoints[:-1], breakpoints[1:]):\n",
    "                new_paths.append(path[b+1:c+2])\n",
    "    else:\n",
    "        new_paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = new_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_paths(paths, corner_pairs, corner_positions, curve_data, curve_distances):\n",
    "new_paths = []\n",
    "path_points = []\n",
    "path_distances = []\n",
    "path_params = []\n",
    "path_knots = []\n",
    "avg_len = np.linalg.norm(np.array(corner_positions)[\n",
    "    np.array(corner_pairs)[:,0]] - np.array(corner_positions)[\n",
    "    np.array(corner_pairs)[:,1]], axis=1).mean()\n",
    "for path in paths:\n",
    "    path_len = len(path)\n",
    "#     print(len(path))\n",
    "    path_pairs = list(zip(path, path[1:]))\n",
    "    current_points = []\n",
    "    current_distances = []\n",
    "    current_params = []\n",
    "    current_knots = []\n",
    "    knot_param = 0\n",
    "    for i,current_pair in enumerate(path_pairs):\n",
    "        endpoints = np.array(corner_positions)[np.array(current_pair)]\n",
    "        ind = np.where(np.isin(corner_pairs, current_pair).sum(1) == 2)[0][0]\n",
    "        if len(path) < 3 and len(curve_data[ind]) < 5:\n",
    "            break\n",
    "        cosines = np.dot(curve_data[ind] - endpoints[0], endpoints[1] - endpoints[0]) / np.linalg.norm(endpoints[1] - endpoints[0])\n",
    "        truncate = np.logical_and(cosines >= 0, cosines <= np.linalg.norm(endpoints[1] - endpoints[0]))\n",
    "\n",
    "        param = cosines[truncate]\n",
    "\n",
    "        argsort = np.argsort(param)\n",
    "        if len(current_params) == 0:\n",
    "            max_param = 0\n",
    "        else:\n",
    "            max_param = max(np.concatenate(current_params).max(),knot_param) \n",
    "        knot_param += np.linalg.norm(endpoints[1] - endpoints[0])\n",
    "        if len(param[argsort]) > 0:\n",
    "            current_params.append(param[argsort] + max_param)\n",
    "            current_points.append(curve_data[ind][truncate][argsort])\n",
    "            current_distances.append(curve_distances[ind][truncate][argsort])\n",
    "    if len(path) < 3 and len(curve_data[ind]) < 5:\n",
    "            continue\n",
    "\n",
    "#     print(len(current_points))\n",
    "    try:\n",
    "        \n",
    "        path_points.append(np.concatenate(current_points))\n",
    "        path_distances.append(np.concatenate(current_distances))\n",
    "        path_params.append(np.concatenate(current_params))\n",
    "        path_knots.append(np.linspace(0,np.concatenate(current_params).max(), max(5, int(len(current_points)/2.))))\n",
    "        new_paths.append(path)\n",
    "    except:\n",
    "        continue\n",
    "#     return path_points, path_distances, path_params, path_knots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = new_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_angles = []\n",
    "for path in paths:\n",
    "    path_pairs = list(zip(path, path[1:]))\n",
    "    current_angles = []\n",
    "    if len(path_pairs) == 1:\n",
    "        path_angles.append(None)\n",
    "        continue\n",
    "    for path_node in range(1,len(path) - 1):\n",
    "        vec1 = np.array(corner_positions)[path[path_node]] - np.array(corner_positions)[path[0]]\n",
    "        vec2 = np.array(corner_positions)[path[-1]] - np.array(corner_positions)[path[path_node]]\n",
    "        vec1 /= np.linalg.norm(vec1)\n",
    "        vec2 /= np.linalg.norm(vec2)\n",
    "        current_angles.append(np.dot(vec1, vec2))\n",
    "    path_angles.append(current_angles)\n",
    "    \n",
    "straight = np.zeros((len(paths)))\n",
    "for i in range(len(paths)):\n",
    "    if len(paths[i]) == 2:\n",
    "        straight[i] = 1\n",
    "    if path_angles[i] is not None:\n",
    "        if (np.array(path_angles[i]) >= straight_threshold).all():\n",
    "            straight[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacing_weights(linspace):\n",
    "    dists = linspace[1:]-linspace[:-1]\n",
    "    return np.array( [dists[0]*0.5] + \n",
    "                       [(dists[i]+dists[i+1])*0.5 for i in range(0,len(dists)-1)]\n",
    "                    +[dists[-1]*0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def err(c, points, distances, u, t, k, endpoints):\n",
    "    c = c.reshape(3,-1)\n",
    "    eval_spline = np.array(splev(u, (t,c,k))).T\n",
    "    proj_distances = np.linalg.norm(points - eval_spline, axis=1)\n",
    "    residual = np.mean(np.abs(proj_distances - distances)) + 0.1*proj_distances.mean()\n",
    "    return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start splining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:04<00:00,  2.94it/s]\n"
     ]
    }
   ],
   "source": [
    "inds = []\n",
    "print('start splining')        \n",
    "tcks = []\n",
    "errs = []\n",
    "straight_lines = []\n",
    "closed_tcks = []\n",
    "for i in tqdm(range(len(paths))):\n",
    "    if len(path_params[i]) < 2:\n",
    "        continue\n",
    "    linspace = (path_params[i] - path_knots[i].min()) / (path_knots[i].max() - path_knots[i].min())\n",
    "    where = np.unique(linspace, return_index=True)[1]\n",
    "    linspace = linspace[where]\n",
    "    argsort = np.argsort(linspace)\n",
    "    linspace = linspace[argsort]\n",
    "    path_points_current = path_points[i][where][argsort]\n",
    "    knots = np.sort(((path_knots[i] - path_knots[i].min()) / (path_knots[i].max() - path_knots[i].min())))\n",
    "    knots_as_needed = np.zeros((6+len(knots)))\n",
    "    knots_as_needed[-3:] = 1\n",
    "    knots_as_needed[3:-3] = knots\n",
    "    weights = 1 - path_distances[i][where][argsort]\n",
    "    space_weights = spacing_weights(linspace)\n",
    "    endpoints = np.array(corner_positions)[np.array(paths[i])[[0,-1]]]\n",
    "    if straight[i] == 1:\n",
    "        straight_lines.append(endpoints)\n",
    "        continue\n",
    "    inds.append(i)\n",
    "    try:\n",
    "        (t,c0,k), u = splprep(u=linspace, x=path_points_current.T, w=space_weights, task=-1, t=knots_as_needed)\n",
    "    except:\n",
    "        continue\n",
    "    con = ({'type': 'eq',\n",
    "           'fun': lambda c: (splev(0, (t, c.reshape(3,-1), k))-endpoints[0])\n",
    "           },\n",
    "           {'type': 'eq',\n",
    "           'fun': lambda c: (splev(1, (t, c.reshape(3,-1), k))-endpoints[1])\n",
    "           })\n",
    "    opt = minimize(err, np.array(c0).flatten(), (path_points_current, (1-weights), u, t, k, endpoints),\n",
    "                  constraints=con, tol=1e-10)\n",
    "    copt = opt.x\n",
    "    tcks.append((t, copt.reshape(3,-1), k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_paths = []\n",
    "path_points = []\n",
    "path_distances = []\n",
    "path_params = []\n",
    "path_knots = []\n",
    "avg_len = np.linalg.norm(np.array(corner_positions)[\n",
    "    np.array(corner_pairs)[:,0]] - np.array(corner_positions)[\n",
    "    np.array(corner_pairs)[:,1]], axis=1).mean()\n",
    "for path in closed_paths:\n",
    "    path_len = len(path)\n",
    "#     print(len(path))\n",
    "    path_pairs = list(zip(path, path[1:]))\n",
    "    current_points = []\n",
    "    current_distances = []\n",
    "    current_params = []\n",
    "    current_knots = []\n",
    "    knot_param = 0\n",
    "    for i,current_pair in enumerate(path_pairs):\n",
    "        endpoints = np.array(corner_positions)[np.array(current_pair)]\n",
    "        ind = np.where(np.isin(corner_pairs, current_pair).sum(1) == 2)[0][0]\n",
    "        if len(path) < 3 and len(curve_data[ind]) < 5:\n",
    "            break\n",
    "        cosines = np.dot(curve_data[ind] - endpoints[0], endpoints[1] - endpoints[0]) / np.linalg.norm(endpoints[1] - endpoints[0])\n",
    "        truncate = np.logical_and(cosines >= 0, cosines <= np.linalg.norm(endpoints[1] - endpoints[0]))\n",
    "\n",
    "        param = cosines[truncate]\n",
    "\n",
    "        argsort = np.argsort(param)\n",
    "        if len(current_params) == 0:\n",
    "            max_param = 0\n",
    "        else:\n",
    "            max_param = max(np.concatenate(current_params).max(),knot_param) \n",
    "        knot_param += np.linalg.norm(endpoints[1] - endpoints[0])\n",
    "        if len(param[argsort]) > 0:\n",
    "            current_params.append(param[argsort] + max_param)\n",
    "            current_points.append(curve_data[ind][truncate][argsort])\n",
    "            current_distances.append(curve_distances[ind][truncate][argsort])\n",
    "    if len(path) < 3 and len(curve_data[ind]) < 5:\n",
    "            continue\n",
    "#     new_paths.append(path)\n",
    "    path_points.append(np.concatenate(current_points))\n",
    "    path_distances.append(np.concatenate(current_distances))\n",
    "    path_params.append(np.concatenate(current_params))\n",
    "    path_knots.append(np.linspace(0,np.concatenate(current_params).max(),6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(closed_paths) > 0:\n",
    "    print('splining closed curves')\n",
    "    \n",
    "    for i in tqdm(range(len(closed_paths))):\n",
    "        linspace = (path_params[i] - path_knots[i].min()) / (path_knots[i].max() - path_knots[i].min())\n",
    "        where = np.unique(linspace, return_index=True)[1]\n",
    "        linspace = linspace[where]\n",
    "        argsort = np.argsort(linspace)\n",
    "        linspace = linspace[argsort]\n",
    "        path_points_current = path_points[i][where][argsort]\n",
    "        knots = np.sort(((path_knots[i] - path_knots[i].min()) / (path_knots[i].max() - path_knots[i].min())))\n",
    "        knots_as_needed = np.zeros((6+len(knots)))\n",
    "        knots_as_needed[-3:] = 1\n",
    "        knots_as_needed[3:-3] = knots\n",
    "        weights = 1 - path_distances[i][where][argsort]\n",
    "        space_weights = spacing_weights(linspace)\n",
    "\n",
    "        (t,c0,k), u = splprep(u=linspace, x=path_points_current.T, w=space_weights, task=-1, t=knots_as_needed)\n",
    "        con = ({'type': 'eq',\n",
    "               'fun': lambda c: (np.array(splev(0, (t, c.reshape(3,-1), k)))-np.array(splev(1, (t, c.reshape(3,-1), k))))\n",
    "               },\n",
    "               {'type': 'eq',\n",
    "               'fun': lambda c: (np.sum(np.abs(np.array(splev(0, (t, c.reshape(3,-1), k), der=1))-np.array(splev(1, (t, c.reshape(3,-1), k), der=1)))))\n",
    "               }\n",
    "              )\n",
    "        opt = minimize(err, np.array(c0).flatten(), (path_points_current, 1-weights, u, t, k, endpoints),\n",
    "                      constraints=con)\n",
    "        copt = opt.x\n",
    "        closed_tcks.append((t, copt.reshape(3,-1), k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closed_tcks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f0805fb77e478fbab7c7e102f64e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = k3d.plot()\n",
    "\n",
    "cmap=k3d.colormaps.matplotlib_color_maps.plasma_r\n",
    "colors = k3d.helpers.map_colors(\n",
    "                distances, cmap, [distances.min(), distances.max()]\n",
    "            ).astype(np.uint32)\n",
    "\n",
    "k3d_points = k3d.points(points, point_size=DISPLAY_RES, opacity=0.1, shader='3d', name='sharp_points')\n",
    "plot += k3d_points\n",
    "\n",
    "for i in range(len(straight_lines)):\n",
    "    spline = k3d.line(straight_lines[i], color=0xff0000, width=DISPLAY_RES-0.015, name='line')\n",
    "    plot += spline\n",
    "\n",
    "for i in range(len(tcks)):\n",
    "    spline = k3d.points(np.array(splev(np.linspace(0,1,2500), tcks[i])).T, color=0xff0000, point_size=DISPLAY_RES-0.015, shader='flat', name='default')\n",
    "    plot += spline\n",
    "    \n",
    "for i in range(len(closed_tcks)):    \n",
    "    spline = k3d.points(np.array(splev(np.linspace(0,1,2500), closed_tcks[i])).T, color=0xff0000, point_size=DISPLAY_RES-0.015, shader='flat', name='spline')\n",
    "    plot += spline\n",
    "\n",
    "plot.display()\n",
    "\n",
    "with open(meta_exp_name+'/'+folder_name+'/'+'vectorization_'+curve_extraction_mode+'.html', 'w') as f:\n",
    "    f.write(plot.get_snapshot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def write_results(folder_name,straight_lines,tcks,closed_tcks):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    with open(folder_name + '/vectorization_line.txt', \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(straight_lines, fp)\n",
    "    with open(folder_name+'/vectorization_open_curves.txt', \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(tcks, fp)\n",
    "    with open(folder_name+'/vectorization_closed_curves.txt', \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(closed_tcks, fp)\n",
    "        \n",
    "def read_results(folder_name):\n",
    "    with open(folder_name + '/vectorization_line.txt', \"rb\") as fp:   # Unpickling\n",
    "        straight_lines = pickle.load(fp)\n",
    "    with open(folder_name+'/vectorization_open_curves.txt', \"rb\") as fp:   #Pickling\n",
    "        tcks = pickle.load(fp)\n",
    "    with open(folder_name+'/vectorization_closed_curves.txt', \"rb\") as fp:   #Pickling\n",
    "        closed_tcks = pickle.load(fp)\n",
    "    return straight_lines,tcks,closed_tcks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results(meta_exp_name+'/'+folder_name+'/raw_vectorization/',straight_lines,tcks,closed_tcks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_calculating(pred_sampling_small,inp_sampling_small,blur=0.05):\n",
    "    loss = SamplesLoss(loss=\"sinkhorn\",backend = \"tensorized\",blur=blur)\n",
    "    sink_ls = loss(pred_sampling_small.cuda(), inp_sampling_small.cuda())\n",
    "\n",
    "    loss = SamplesLoss(loss=\"energy\",backend = \"tensorized\",blur=blur)\n",
    "    en_ls = loss(pred_sampling_small.cuda(), inp_sampling_small.cuda())\n",
    "\n",
    "    loss = SamplesLoss(loss=\"gaussian\",backend = \"tensorized\",blur=blur)\n",
    "    gaus_ls = loss(pred_sampling_small.cuda(), inp_sampling_small.cuda())\n",
    "\n",
    "    loss = SamplesLoss(loss=\"laplacian\",backend = \"tensorized\",blur=blur)\n",
    "    lap_ls = loss(pred_sampling_small.cuda(), inp_sampling_small.cuda())\n",
    "\n",
    "    return sink_ls, en_ls, gaus_ls, lap_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(folder_name, file_name, CD_gt_to_pred, CD_pred_to_gt,chamf_l, sink_ls, en_ls, gaus_ls, lap_ls):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    with open(folder_name +'/'+'metrics_'+ file_name + curve_extraction_mode + '.txt', 'w') as f:\n",
    "        f.write('CD_gt_to_pred')\n",
    "        f.write('\\n')\n",
    "        f.write(str(CD_gt_to_pred.item()))\n",
    "        f.write('\\n')\n",
    "        f.write('CD_pred_to_gt')\n",
    "        f.write('\\n')\n",
    "        f.write(str(CD_pred_to_gt.item()))\n",
    "        f.write('\\n')\n",
    "        f.write('Chamfer loss')\n",
    "        f.write('\\n')\n",
    "        f.write(str(chamf_l))\n",
    "        f.write('\\n')\n",
    "        f.write('Energy loss')\n",
    "        f.write('\\n')\n",
    "        f.write(str(en_ls.item()))\n",
    "        f.write('\\n')\n",
    "        f.write('Sinkhorn loss')\n",
    "        f.write('\\n')\n",
    "        f.write(str(sink_ls.item()))\n",
    "        f.write('\\n')\n",
    "        f.write('Gaussian loss')\n",
    "        f.write('\\n')\n",
    "        f.write(str(gaus_ls.item()))\n",
    "        f.write('\\n')\n",
    "        f.write('Laplassian loss')\n",
    "        f.write('\\n')\n",
    "        f.write(str(lap_ls.item()))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "NO_GT=False\n",
    "# load GT curves as segments\n",
    "gt_edges = []\n",
    "if os.path.exists('parametric_labeling/{file}__parametric.txt'.format(file=_id[9:])):\n",
    "    with open('parametric_labeling/{file}__parametric.txt'.format(file=_id[9:])) as f:\n",
    "        for line in f:\n",
    "            curve_id, curve_type, edges = line.split(' ', maxsplit=2)\n",
    "            gt_edges.append(np.array([float(v) for v in edges.split()]).reshape((-1, 2, 3)))\n",
    "else:\n",
    "    NO_GT = True\n",
    "    print('No GT!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample GT edges\n",
    "if not NO_GT:\n",
    "    gt_sampling = []\n",
    "    for edge in np.concatenate(gt_edges):\n",
    "        num = int(np.linalg.norm(edge[1] - edge[0]) // 0.001)\n",
    "        if num > 0:\n",
    "            linspace = np.linspace(0, 1, num)\n",
    "        #     break\n",
    "            gt_sampling.append(linspace[:,None] * edge[0] + (1 - linspace)[:,None] * edge[1])\n",
    "        else:\n",
    "            gt_sampling.append(edge)\n",
    "    gt_sampling = np.concatenate(gt_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_sampling_points(straight_lines,tcks,closed_tcks):\n",
    "    pred_sampling = []\n",
    "\n",
    "    for i in range(len(straight_lines)):\n",
    "        edge = straight_lines[i]\n",
    "        num = int(np.linalg.norm(edge[1] - edge[0]) // 0.001)\n",
    "        if num > 0:\n",
    "            linspace = np.linspace(0, 1, num)\n",
    "        #     break\n",
    "            pred_sampling.append(linspace[:,None] * edge[0] + (1 - linspace)[:,None] * edge[1])\n",
    "        else:\n",
    "            pred_sampling.append(edge)\n",
    "\n",
    "    for i in range(len(tcks)):\n",
    "        pred_sampling.append(np.array(splev(np.linspace(0,1,5000), tcks[i])).T)\n",
    "\n",
    "    for i in range(len(closed_tcks)):    \n",
    "        pred_sampling.append(np.array(splev(np.linspace(0,1,5000), closed_tcks[i])).T)\n",
    "    return pred_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_calculation(pred_sampling,gt_sampling,blur=0.05):\n",
    "  \n",
    "    CD_gt_to_pred = (cKDTree(pred_sampling).query(gt_sampling, 1)[0]**2).mean()\n",
    "    CD_pred_to_gt = (cKDTree(gt_sampling).query(pred_sampling, 1)[0]**2).mean()\n",
    "    \n",
    "    pred_sampling = torch.tensor(pred_sampling)[None]\n",
    "    gt_sampling = torch.tensor(gt_sampling)[None]\n",
    "    chamf_l = chamfer_distance(pred_sampling.clone().cuda().detach().float(), gt_sampling.clone().cuda().detach().float())\n",
    "    \n",
    "    if pred_sampling[0].shape[0]>10000:\n",
    "        sub_idx = pcu.downsample_point_cloud_poisson_disk(np.array(pred_sampling[0]), num_samples=10000)\n",
    "        # Use the indices to get the sample positions and normals\n",
    "        pred_sampling_small = pred_sampling[:,sub_idx]\n",
    "    else:\n",
    "        pred_sampling_small =  pred_sampling\n",
    "    \n",
    "    if gt_sampling[0].shape[0]>10000:\n",
    "        sub_idx = pcu.downsample_point_cloud_poisson_disk(np.array(gt_sampling[0]), num_samples=10000)\n",
    "        # Use the indices to get the sample positions and normals\n",
    "        gt_sampling_small = gt_sampling[:,sub_idx]\n",
    "    else:\n",
    "        gt_sampling_small = gt_sampling\n",
    "    sink_ls, en_ls, gaus_ls, lap_ls = loss_calculating(pred_sampling_small,gt_sampling_small,blur=blur)\n",
    "    return CD_gt_to_pred, CD_pred_to_gt,chamf_l, sink_ls, en_ls, gaus_ls, lap_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sampling = pred_sampling_points(straight_lines,tcks,closed_tcks)\n",
    "pred_sampling = np.concatenate(pred_sampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not NO_GT:\n",
    "    CD_gt_to_pred, CD_pred_to_gt,chamf_l, sink_ls, en_ls, gaus_ls, lap_ls  = metric_calculation(pred_sampling,gt_sampling)\n",
    "    _, _,chamf_l, sink_ls_bl02, en_ls_bl02, gaus_ls_bl02, lap_ls_bl02  = metric_calculation(pred_sampling,gt_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not NO_GT:\n",
    "    write_to_file(meta_exp_name+'/'+folder_name+'/metrics/','raw_vectorization',CD_gt_to_pred, CD_pred_to_gt,chamf_l, sink_ls, en_ls, gaus_ls, lap_ls)\n",
    "    df = pd.DataFrame([[folder_name,CD_gt_to_pred.item(),CD_pred_to_gt.item(),chamf_l[0].item(),\n",
    "                        en_ls.item(),sink_ls.item(),\n",
    "                        gaus_ls.item(),lap_ls.item(), en_ls_bl02.item(),sink_ls_bl02.item(),\n",
    "                        gaus_ls_bl02.item(),lap_ls_bl02.item()]],\n",
    "                      index=None,  columns=['Name','CD_gt_to_pred','CD_pred_to_gt','Chamfer_loss',\n",
    "                                            'Energy_loss','Sinkhorn_loss','Gaussian_loss'\n",
    "                                            ,'Laplassian_loss','Energy_loss_blure02','Sinkhorn_loss_blure02','Gaussian_loss_blure02'\n",
    "                                            ,'Laplassian_loss_blure02'])\n",
    "    if not os.path.exists(meta_exp_name+'/metrics/'):\n",
    "        os.makedirs(meta_exp_name+'/metrics/')\n",
    "    \n",
    "    if not os.path.exists(meta_exp_name+'/metrics/'+meta_exp_name + '_raw_vectorization_metrics.csv'):\n",
    "        df.to_csv(meta_exp_name+'/metrics/'+meta_exp_name + '_raw_vectorization_metrics.csv', index = False,sep=';')\n",
    "    else:\n",
    "        df.to_csv(meta_exp_name+'/metrics/'+meta_exp_name + '_raw_vectorization_metrics.csv', mode='a', header=None, index = False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68202e0190b45548a1ae467ef0c0feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save this also\n",
    "if not NO_GT:\n",
    "    plot = k3d.plot()\n",
    "\n",
    "    cmap=k3d.colormaps.matplotlib_color_maps.plasma_r\n",
    "    colors = k3d.helpers.map_colors(\n",
    "                    distances, cmap, [distances.min(), distances.max()]\n",
    "                ).astype(np.uint32)\n",
    "\n",
    "    k3d_points = k3d.points(pred_sampling, point_size=DISPLAY_RES, opacity=0.1, shader='3d', name='sharp_points', color=0xff0000)\n",
    "    plot += k3d_points\n",
    "\n",
    "    k3d_points = k3d.points(gt_sampling, point_size=DISPLAY_RES, opacity=0.1, shader='3d', name='sharp_points')\n",
    "    plot += k3d_points\n",
    "\n",
    "    plot.display()\n",
    "    with open(meta_exp_name+'/'+folder_name+'/'+'vectorization_with_GT_'+curve_extraction_mode+'.html', 'w') as f:\n",
    "        f.write(plot.get_snapshot())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caclulating distance between prediction and input point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "CD_gt_to_pred_mm, CD_pred_to_gt_mm, chamf_l_mm, sink_ls_mm, en_ls_mm, gaus_ls_mm, lap_ls_mm  = metric_calculation(pred_sampling,points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(meta_exp_name+'/'+folder_name+'/metrics/','chosing_raw_vectorization',CD_gt_to_pred_mm, CD_pred_to_gt_mm, chamf_l_mm, sink_ls_mm, en_ls_mm, gaus_ls_mm, lap_ls_mm)\n",
    "\n",
    "df = pd.DataFrame([[folder_name,chamf_l_mm[0].item(),en_ls_mm.item(),sink_ls_mm.item(),gaus_ls_mm.item(),lap_ls_mm.item()]],\n",
    "                  index=None,  columns=['Name','Chamfer_loss','Energy_loss','Sinkhorn_loss','Gaussian_loss','Laplassian_loss'])\n",
    "\n",
    "if not os.path.exists(meta_exp_name+'/metrics/'):\n",
    "    os.makedirs(meta_exp_name+'/metrics/')\n",
    "    \n",
    "if not os.path.exists(meta_exp_name+'/metrics/' + meta_exp_name + '_chosing_raw_vectorization_metrics.csv'):\n",
    "    df.to_csv(meta_exp_name+'/metrics/'+meta_exp_name + '_chosing_raw_vectorization_metrics.csv', index = False,sep=';')\n",
    "else:\n",
    "    df.to_csv(meta_exp_name+'/metrics/'+meta_exp_name + '_chosing_raw_vectorization_metrics.csv', mode='a', header=None, index = False,sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# straight_lines,tcks,closed_tcks = read_results(meta_exp_name+'/'+folder_name+'/raw_vectorization/')\n",
    "# cut_metric ='fscore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.00001\n",
    "\n",
    "def fscorebeta(dist1, dist2, threshold=0.005, beta = 0.5):\n",
    "    # From here https://github.com/ThibaultGROUEIX/ChamferDistancePytorch/blob/master/fscore.py\n",
    "    \"\"\"\n",
    "    Calculates the F-score between two point clouds with the corresponding threshold value.\n",
    "    \n",
    "    :param dist1: Batch, N-Points\n",
    "    :param dist2: Batch, N-Points\n",
    "    :param th: float\n",
    "    :return: fscore, precision, recall\n",
    "    \"\"\"\n",
    "    # NB : In this depo, dist1 and dist2 are squared pointcloud euclidean distances, so you should adapt the threshold accordingly.\n",
    "    precision_1 = torch.mean((dist1 < threshold).float(), dim=1)\n",
    "    precision_2 = torch.mean((dist2 < threshold).float(), dim=1)\n",
    "    fscore = (1 + beta*beta) * precision_1 * precision_2 / (beta*beta*precision_1 + precision_2)\n",
    "    fscore[torch.isnan(fscore)] = 0\n",
    "    return fscore, precision_1, precision_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sampling = pred_sampling_points(straight_lines,tcks,closed_tcks)\n",
    "pred_sampling = np.concatenate(pred_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cut_metric =='fscore' or cut_metric =='precision':\n",
    "    CD_gt_to_pred = (cKDTree(pred_sampling).query(points, 1)[0]**2)\n",
    "    CD_pred_to_gt = (cKDTree(points).query(pred_sampling, 1)[0]**2)\n",
    "else:\n",
    "    pred_sampling = torch.tensor(pred_sampling)[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cut_metric =='fscore'  or cut_metric =='precision':\n",
    "    fsc_max,pred_1_max,pred_2_max = fscorebeta(torch.tensor(CD_pred_to_gt)[None],torch.tensor(CD_gt_to_pred)[None]) \n",
    "elif cut_metric == 'chamfer_dist':\n",
    "    ch_dst_min = chamf_l_mm[0].item()\n",
    "elif cut_metric == 'sinkhorn':\n",
    "    sink_dst_min = sink_ls_mm.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9802]) tensor([0.9936]) tensor([0.9299])\n",
      "tensor([0.9753]) tensor([0.9936]) tensor([0.9084])\n",
      "tensor([0.9860]) tensor([0.9937]) tensor([0.9561])\n",
      "tensor([0.9788]) tensor([0.9936]) tensor([0.9236])\n",
      "tensor([0.9817]) tensor([0.9936]) tensor([0.9369])\n",
      "tensor([0.9751]) tensor([0.9936]) tensor([0.9077])\n",
      "tensor([0.9790]) tensor([0.9936]) tensor([0.9245])\n",
      "tensor([0.9643]) tensor([1.]) tensor([0.8438])\n",
      "tensor([0.9712]) tensor([0.9929]) tensor([0.8931])\n",
      "tensor([0.9577]) tensor([0.9929]) tensor([0.8387])\n",
      "tensor([0.9762]) tensor([0.9929]) tensor([0.9143])\n",
      "tensor([0.9723]) tensor([0.9929]) tensor([0.8976])\n",
      "tensor([0.9755]) tensor([0.9929]) tensor([0.9112])\n",
      "tensor([0.9754]) tensor([0.9929]) tensor([0.9110])\n",
      "Get rid of: 0\n"
     ]
    }
   ],
   "source": [
    "it=0\n",
    "curve_cuted =0\n",
    "while it<len(straight_lines)+len(tcks)+len(closed_tcks):\n",
    "    pred_sampling = []\n",
    "    tks =  copy.deepcopy(tcks)\n",
    "    str_lines = copy.deepcopy(straight_lines)\n",
    "    closed_tks = copy.deepcopy(closed_tcks)\n",
    "    if it < len(straight_lines):\n",
    "        str_lines.pop(it)\n",
    "    elif it < len(straight_lines)+len(tcks):\n",
    "        tks.pop(it-len(straight_lines))   \n",
    "    else:\n",
    "        closed_tks.pop(it-len(straight_lines)-len(tcks))\n",
    "    \n",
    "    if len(str_lines)+len(tks)+len(closed_tks)==0:\n",
    "        straight_lines = copy.deepcopy(str_lines)\n",
    "        tcks = copy.deepcopy(tks)\n",
    "        closed_tcks = copy.deepcopy(closed_tks)\n",
    "        curve_cuted+=1\n",
    "        break\n",
    "        \n",
    "    pred_sampling = pred_sampling_points(str_lines,tks,closed_tks)\n",
    "    pred_sampling = np.concatenate(pred_sampling)\n",
    "    \n",
    "    if cut_metric =='fscore' or cut_metric =='precision':\n",
    "        CD_gt_to_pred = (cKDTree(pred_sampling).query(points, 1)[0]**2)\n",
    "        CD_pred_to_gt = (cKDTree(points).query(pred_sampling, 1)[0]**2)\n",
    "        fsc,pred_1,pred_2  = fscorebeta(torch.tensor(CD_pred_to_gt)[None],torch.tensor(CD_gt_to_pred)[None]) \n",
    "        print(fsc,pred_1,pred_2)\n",
    "        if cut_metric =='precision':\n",
    "            if pred_1 > pred_1_max:\n",
    "                fsc_max = fsc\n",
    "                pred_1_max = pred_1\n",
    "                pred_2_max = pred_2\n",
    "                straight_lines = copy.deepcopy(str_lines)\n",
    "                tcks = copy.deepcopy(tks)\n",
    "                closed_tcks = copy.deepcopy(closed_tks)\n",
    "                curve_cuted+=1\n",
    "            else:\n",
    "                it+=1\n",
    "        else:\n",
    "            if fsc > fsc_max:\n",
    "                fsc_max = fsc\n",
    "                pred_2_max = pred_2\n",
    "                pred_1_max = pred_1\n",
    "                straight_lines = copy.deepcopy(str_lines)\n",
    "                tcks = copy.deepcopy(tks)\n",
    "                closed_tcks = copy.deepcopy(closed_tks)\n",
    "                curve_cuted+=1\n",
    "            else:\n",
    "                it+=1\n",
    "    elif cut_metric == 'chamfer_dist':\n",
    "        pred_sampling = torch.tensor(pred_sampling)[None]\n",
    "        chamf_l_mm = chamfer_distance(pred_sampling.clone().cuda().detach().float(), torch.tensor(points)[None].clone().cuda().detach().float())\n",
    "        if chamf_l_mm[0].item()<=ch_dst_min:\n",
    "            ch_dst_min = chamf_l_mm[0].item()\n",
    "            straight_lines = copy.deepcopy(str_lines)\n",
    "            tcks = copy.deepcopy(tks)\n",
    "            closed_tcks = copy.deepcopy(closed_tks)\n",
    "            curve_cuted+=1\n",
    "        else:\n",
    "            it+=1\n",
    "    elif cut_metric == 'sinkhorn':\n",
    "        pred_sampling = torch.tensor(pred_sampling)[None]\n",
    "        if pred_sampling[0].shape[0]>10000:\n",
    "            sub_idx = pcu.downsample_point_cloud_poisson_disk(np.array(pred_sampling[0]), num_samples=10000)\n",
    "            pred_sampling_small = pred_sampling[:,sub_idx]\n",
    "        else:\n",
    "            pred_sampling_small =  pred_sampling\n",
    "        loss = SamplesLoss(loss=\"sinkhorn\",backend = \"tensorized\")\n",
    "        sink_ls_mm = loss(pred_sampling_small.cuda(), inp_sampling_small.cuda())\n",
    "        if  sink_ls_mm.item()<=sink_dst_min:\n",
    "            sink_dst_min= sink_ls_mm.item()\n",
    "            straight_lines = copy.deepcopy(str_lines)\n",
    "            tcks = copy.deepcopy(tks)\n",
    "            closed_tcks = copy.deepcopy(closed_tks)\n",
    "            curve_cuted+=1\n",
    "        else:\n",
    "            it+=1\n",
    "print('Get rid of:', curve_cuted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2b218a719a46239bd5d80ebf1623da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = k3d.plot()\n",
    "\n",
    "cmap=k3d.colormaps.matplotlib_color_maps.plasma_r\n",
    "colors = k3d.helpers.map_colors(\n",
    "                distances, cmap, [distances.min(), distances.max()]\n",
    "            ).astype(np.uint32)\n",
    "\n",
    "k3d_points = k3d.points(points, point_size=DISPLAY_RES, opacity=0.1, shader='3d', name='sharp_points')\n",
    "plot += k3d_points\n",
    "\n",
    "for i in range(len(straight_lines)):\n",
    "    spline = k3d.line(straight_lines[i], color=0xff0000, width=DISPLAY_RES-0.015, name='line')\n",
    "    plot += spline\n",
    "\n",
    "for i in range(len(tcks)):\n",
    "    spline = k3d.points(np.array(splev(np.linspace(0,1,2500), tcks[i])).T, color=0xff0000, point_size=DISPLAY_RES-0.015, shader='flat', name='default')\n",
    "    plot += spline\n",
    "    \n",
    "for i in range(len(closed_tcks)):    \n",
    "    spline = k3d.points(np.array(splev(np.linspace(0,1,2500), closed_tcks[i])).T, color=0xff0000, point_size=DISPLAY_RES-0.015, shader='flat', name='spline')\n",
    "    plot += spline\n",
    "\n",
    "plot.display()\n",
    "\n",
    "with open(meta_exp_name+'/'+folder_name+'/'+'pretified_vectorization_'+curve_extraction_mode+'.html', 'w') as f:\n",
    "    f.write(plot.get_snapshot())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing metrics and curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample GT edges\n",
    "if not NO_GT:\n",
    "    gt_sampling = []\n",
    "    for edge in np.concatenate(gt_edges):\n",
    "        num = int(np.linalg.norm(edge[1] - edge[0]) // 0.001)\n",
    "        if num > 0:\n",
    "            linspace = np.linspace(0, 1, num)\n",
    "        #     break\n",
    "            gt_sampling.append(linspace[:,None] * edge[0] + (1 - linspace)[:,None] * edge[1])\n",
    "        else:\n",
    "            gt_sampling.append(edge)\n",
    "    gt_sampling = np.concatenate(gt_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results(meta_exp_name+'/'+folder_name+'/pretified_vectorization/',straight_lines,tcks,closed_tcks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sampling = pred_sampling_points(straight_lines,tcks,closed_tcks)\n",
    "pred_sampling = np.concatenate(pred_sampling)\n",
    "if not NO_GT:\n",
    "    CD_gt_to_pred, CD_pred_to_gt,chamf_l, sink_ls, en_ls, gaus_ls, lap_ls  = metric_calculation(pred_sampling,gt_sampling)\n",
    "    _, _,chamf_l, sink_ls_bl02, en_ls_bl02, gaus_ls_bl02, lap_ls_bl02  = metric_calculation(pred_sampling,gt_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not NO_GT:\n",
    "    write_to_file(meta_exp_name+'/'+folder_name+'/metrics/','pretified_vectorization',CD_gt_to_pred, CD_pred_to_gt,chamf_l, sink_ls, en_ls, gaus_ls, lap_ls)\n",
    "    df = pd.DataFrame([[folder_name,CD_gt_to_pred.item(),CD_pred_to_gt.item(),chamf_l[0].item(),\n",
    "                        en_ls.item(),sink_ls.item(),\n",
    "                        gaus_ls.item(),lap_ls.item(), en_ls_bl02.item(),sink_ls_bl02.item(),\n",
    "                        gaus_ls_bl02.item(),lap_ls_bl02.item()]],\n",
    "                      index=None,  columns=['Name','CD_gt_to_pred','CD_pred_to_gt','Chamfer_loss',\n",
    "                                            'Energy_loss','Sinkhorn_loss','Gaussian_loss'\n",
    "                                            ,'Laplassian_loss','Energy_loss_blure02','Sinkhorn_loss_blure02','Gaussian_loss_blure02'\n",
    "                                            ,'Laplassian_loss_blure02'])\n",
    "    if not os.path.exists(meta_exp_name+'/metrics/'):\n",
    "        os.makedirs(meta_exp_name+'/metrics/')\n",
    "    \n",
    "    if not os.path.exists(meta_exp_name+'/metrics/'+meta_exp_name + '_pretified_vectorization_metrics.csv'):\n",
    "        df.to_csv(meta_exp_name+'/metrics/'+meta_exp_name + '_pretified_vectorization_metrics.csv', index = False,sep=';')\n",
    "    else:\n",
    "        df.to_csv(meta_exp_name+'/metrics/'+meta_exp_name + '_pretified_vectorization_metrics.csv', mode='a', header=None, index = False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "CD_gt_to_pred_mm, CD_pred_to_gt_mm, chamf_l_mm, sink_ls_mm, en_ls_mm, gaus_ls_mm, lap_ls_mm  = metric_calculation(pred_sampling,points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(meta_exp_name+'/'+folder_name+'/metrics/','chosing_pretified_vectorization',CD_gt_to_pred_mm, CD_pred_to_gt_mm, chamf_l_mm, sink_ls_mm, en_ls_mm, gaus_ls_mm, lap_ls_mm)\n",
    "\n",
    "df = pd.DataFrame([[folder_name,chamf_l_mm[0].item(),en_ls_mm.item(),sink_ls_mm.item(),gaus_ls_mm.item(),lap_ls_mm.item()]],\n",
    "                  index=None,  columns=['Name','Chamfer_loss','Energy_loss','Sinkhorn_loss','Gaussian_loss','Laplassian_loss'])\n",
    "\n",
    "if not os.path.exists(meta_exp_name+'/metrics/'):\n",
    "    os.makedirs(meta_exp_name+'/metrics/')\n",
    "    \n",
    "if not os.path.exists(meta_exp_name+'/metrics/' + meta_exp_name + '_chosing_pretified_vectorization_metrics.csv'):\n",
    "    df.to_csv(meta_exp_name+'/metrics/'+meta_exp_name + '_chosing_pretified_vectorization_metrics.csv', index = False,sep=';')\n",
    "else:\n",
    "    df.to_csv(meta_exp_name+'/metrics/'+meta_exp_name + '_chosing_pretified_vectorization_metrics.csv', mode='a', header=None, index = False,sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small lines cutting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curve_length(curve,devision_points_count=200):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        curve - curve in as in scipy splprep\n",
    "        devision_points_count - the amount of points on curve\n",
    "    output:\n",
    "        length of the curve\n",
    "    \"\"\"\n",
    "    ar = np.array(splev(np.linspace(0,1,devision_points_count), curve)).T\n",
    "    return np.sum(np.sqrt(np.sum((ar[:-1]-ar[1:])**2,axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding curves endpoints as lines for constructing connection graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_lines = copy.deepcopy(straight_lines) \n",
    "for it in tcks:\n",
    "    ar = np.array(splev(np.linspace(0,1,2), it)).T\n",
    "    str_lines.append(ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing graph where vertex are primitives and edges between vertex exist if primitives share endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(len(str_lines)))\n",
    "\n",
    "for i,line in enumerate(str_lines):\n",
    "        for j,line_2 in  enumerate(str_lines):\n",
    "            if j>i:\n",
    "                if np.allclose(line[0],line_2[0]) or np.allclose(line[0],line_2[1]) or np.allclose(line[1],line_2[0]) or np.allclose(line[1],line_2[1]):\n",
    "                    G.add_edge(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting connected subgraphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = [G.subgraph(c).copy() for c in nx.connected_components(G)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating for each length of  conected segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = []\n",
    "for sg in S:\n",
    "    gr_len=0\n",
    "    for node in sg.nodes():\n",
    "        if node<len(straight_lines):\n",
    "            #for lines\n",
    "            gr_len+=np.sqrt(np.sum((straight_lines[node][0]-straight_lines[node][1])**2))\n",
    "        else:\n",
    "            #for curves\n",
    "            gr_len+=curve_length(tcks[node-len(straight_lines)])\n",
    "    gr.append(gr_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting read of small curves and lines which are in small component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_line_copy = []\n",
    "tcks_copy = []\n",
    "for i,sg in enumerate(S):\n",
    "    if gr[i]<small_lines_clip_primitives_length and len(sg.nodes)<=small_lines_clip_primitive_number:\n",
    "        continue\n",
    "    for node in sg.nodes():\n",
    "        if node<len(straight_lines):\n",
    "            #for lines\n",
    "            str_line_copy.append(straight_lines[node])\n",
    "        else:\n",
    "            #for curves\n",
    "            tcks_copy.append(tcks[node-len(straight_lines)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "straight_lines = str_line_copy\n",
    "tcks = tcks_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506c301dbf344086bfbe0e6000928d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = k3d.plot()\n",
    "\n",
    "cmap=k3d.colormaps.matplotlib_color_maps.plasma_r\n",
    "colors = k3d.helpers.map_colors(\n",
    "                distances, cmap, [distances.min(), distances.max()]\n",
    "            ).astype(np.uint32)\n",
    "\n",
    "k3d_points = k3d.points(points, point_size=DISPLAY_RES, opacity=0.1, shader='3d', name='sharp_points')\n",
    "plot += k3d_points\n",
    "\n",
    "for i in range(len(straight_lines)):\n",
    "    spline = k3d.line(str_line_copy[i], color=0xff0000, width=DISPLAY_RES-0.015, name='line')\n",
    "    plot += spline\n",
    "\n",
    "for i in range(len(tcks)):\n",
    "    spline = k3d.points(np.array(splev(np.linspace(0,1,2500), tcks[i])).T, color=0xff0000, point_size=DISPLAY_RES-0.015, shader='flat', name='default')\n",
    "    plot += spline\n",
    "    \n",
    "for i in range(len(closed_tcks)):    \n",
    "    spline = k3d.points(np.array(splev(np.linspace(0,1,2500), closed_tcks[i])).T, color=0xff0000, point_size=DISPLAY_RES-0.015, shader='flat', name='spline')\n",
    "    plot += spline\n",
    "\n",
    "plot.display()\n",
    "\n",
    "with open(meta_exp_name+'/'+folder_name+'/'+'pretified_vectorization_final_'+curve_extraction_mode+'.html', 'w') as f:\n",
    "    f.write(plot.get_snapshot())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing metrics and curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample GT edges\n",
    "if not NO_GT:\n",
    "    gt_sampling = []\n",
    "    for edge in np.concatenate(gt_edges):\n",
    "        num = int(np.linalg.norm(edge[1] - edge[0]) // 0.001)\n",
    "        if num > 0:\n",
    "            linspace = np.linspace(0, 1, num)\n",
    "        #     break\n",
    "            gt_sampling.append(linspace[:,None] * edge[0] + (1 - linspace)[:,None] * edge[1])\n",
    "        else:\n",
    "            gt_sampling.append(edge)\n",
    "    gt_sampling = np.concatenate(gt_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results(meta_exp_name+'/'+folder_name+'/pretified_vectorization_final/',str_line_copy,tcks,closed_tcks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sampling = pred_sampling_points(straight_lines,tcks,closed_tcks)\n",
    "pred_sampling = np.concatenate(pred_sampling)\n",
    "if not NO_GT:\n",
    "    CD_gt_to_pred, CD_pred_to_gt,chamf_l, sink_ls, en_ls, gaus_ls, lap_ls  = metric_calculation(pred_sampling,gt_sampling)\n",
    "    _, _,chamf_l, sink_ls_bl02, en_ls_bl02, gaus_ls_bl02, lap_ls_bl02  = metric_calculation(pred_sampling,gt_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not NO_GT:\n",
    "    write_to_file(meta_exp_name+'/'+folder_name+'/metrics/','pretified_vectorization_final',CD_gt_to_pred, CD_pred_to_gt,chamf_l, sink_ls, en_ls, gaus_ls, lap_ls)\n",
    "    df = pd.DataFrame([[folder_name,CD_gt_to_pred.item(),CD_pred_to_gt.item(),chamf_l[0].item(),\n",
    "                        en_ls.item(),sink_ls.item(),\n",
    "                        gaus_ls.item(),lap_ls.item(), en_ls_bl02.item(),sink_ls_bl02.item(),\n",
    "                        gaus_ls_bl02.item(),lap_ls_bl02.item()]],\n",
    "                      index=None,  columns=['Name','CD_gt_to_pred','CD_pred_to_gt','Chamfer_loss',\n",
    "                                            'Energy_loss','Sinkhorn_loss','Gaussian_loss'\n",
    "                                            ,'Laplassian_loss','Energy_loss_blure02','Sinkhorn_loss_blure02','Gaussian_loss_blure02'\n",
    "                                            ,'Laplassian_loss_blure02'])\n",
    "    if not os.path.exists(meta_exp_name+'/metrics/'):\n",
    "        os.makedirs(meta_exp_name+'/metrics/')\n",
    "    \n",
    "    if not os.path.exists(meta_exp_name+'/metrics/'+meta_exp_name + '_pretified_vectorization_final_metrics.csv'):\n",
    "        df.to_csv(meta_exp_name+'/metrics/'+meta_exp_name + '_pretified_vectorization_final_metrics.csv', index = False,sep=';')\n",
    "    else:\n",
    "        df.to_csv(meta_exp_name+'/metrics/'+meta_exp_name + '_pretified_vectorization_final_metrics.csv', mode='a', header=None, index = False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "CD_gt_to_pred_mm, CD_pred_to_gt_mm, chamf_l_mm, sink_ls_mm, en_ls_mm, gaus_ls_mm, lap_ls_mm  = metric_calculation(pred_sampling,points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(meta_exp_name+'/'+folder_name+'/metrics/','chosing_pretified_vectorization_final',CD_gt_to_pred_mm, CD_pred_to_gt_mm, chamf_l_mm, sink_ls_mm, en_ls_mm, gaus_ls_mm, lap_ls_mm)\n",
    "\n",
    "df = pd.DataFrame([[folder_name,chamf_l_mm[0].item(),en_ls_mm.item(),sink_ls_mm.item(),gaus_ls_mm.item(),lap_ls_mm.item()]],\n",
    "                  index=None,  columns=['Name','Chamfer_loss','Energy_loss','Sinkhorn_loss','Gaussian_loss','Laplassian_loss'])\n",
    "\n",
    "if not os.path.exists(meta_exp_name+'/metrics/'):\n",
    "    os.makedirs(meta_exp_name+'/metrics/')\n",
    "    \n",
    "if not os.path.exists(meta_exp_name+'/metrics/' + meta_exp_name + '_chosing_pretified_vectorization_final_metrics.csv'):\n",
    "    df.to_csv(meta_exp_name+'/metrics/'+meta_exp_name + '_chosing_pretified_vectorization_final_metrics.csv', index = False,sep=';')\n",
    "else:\n",
    "    df.to_csv(meta_exp_name+'/metrics/'+meta_exp_name + '_chosing_pretified_vectorization_final_metrics.csv', mode='a', header=None, index = False,sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
