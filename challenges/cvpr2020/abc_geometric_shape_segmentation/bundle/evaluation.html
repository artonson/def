<h2>Evaluation</h2>

<p>We supply a training dataset with ground truth segmentation values for training your model, a validation dataset without ground truth segmentation values, for which you can upload your estimations and receive feedback on your performance, as well as a testing dataset without ground truth segmentation values for which you can submit your estimations without immediate feedback. The evaluation on the testing dataset will result in the final score.</p>

<p>One binary (zero or one) segmentation value has to be estimated per point, and all estimated segmentation values for the point cloud have to be written into a text file with the same index as the point cloud file, suffixed by "_target", and in the same order as the points (see example submission in starting kit).</p>

<p>For evaluation, the estimated per-point segmentation values <i>p<sub>PRED</sub></i> are compared to the ground truth segmentations with two measures: the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html">balanced accuracy score</a> and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html">intersection-over-union score</a>. <strong>See our <a href="#learn_the_details-get_starting_kit">starting kit</a> for python evaluation code.</strong></p>

<p>The final reported scores are mean Accuracy and IoU scores computed for each of the point clouds.</p>