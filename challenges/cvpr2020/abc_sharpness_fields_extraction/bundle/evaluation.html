<h2>Evaluation</h2>

<p>We supply a training dataset with ground truth distance field values for training your model, a validation dataset without ground truth distance field values, for which you can upload your estimations and receive feedback on your performance, as well as a testing dataset without ground truth distance field values for which you can submit your estimations without immediate feedback. The evaluation on the testing dataset will result in the final score.</p>

<p>One nonnegative floating-point distance value has to be estimated per point, and all estimated distance values for the point cloud have to be written into a text file with the same index as the point cloud file, suffixed by "_target", and in the same order as the points (see example submission in starting kit).</p>

<p>For evaluation, the estimated per-point distance values <i>d<sub>PRED</sub></i> are first clipped to [0, 1] range by taking <i>CLIP(d<sub>PRED</sub>) = max(0, min(1, d<sub>PRED</sub>))</i> and then compared to the ground truth distances with the root mean squared error (RMSE) evaluation function: <i>SQRT(||d<sub>GT</sub> &mdash; CLIP(d<sub>PRED</sub>)||<sup>2</sup>)</i>, with <i>d<sub>GT</sub></i> as the ground truth distance label (thresholded to 1.0). The scores are calculated as the mean over of all distance values and all models. <strong>See our <a href="#learn_the_details-get_starting_kit">starting kit</a> for python evaluation code.</strong></p>

<p>The final reported score is the mean over RMSE scores computed for each of the point clouds.</p>